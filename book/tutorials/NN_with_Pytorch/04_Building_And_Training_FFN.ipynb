{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a Feed Forward Neural Network in PyTorch\n",
    "\n",
    "In this notebook, we’ll build a simple neural network using PyTorch, train it on the SNOTEL dataset, and evaluate its performance. This hands-on exercise will reinforce our understanding of the PyTorch framework and the steps involved in building and training neural networks on real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "available_device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Available device: {available_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "### Step 1: Load Dataset\n",
    "\n",
    "We'll start by loading the SNOTEL dataset from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_data=pd.read_csv(\"data/clean_data.csv\")\n",
    "snotel_data.info()\n",
    "snotel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Split\n",
    "\n",
    "We’ll split the data into training, validation, and testing sets. Typically, a common split might be 70% training, 15% validation, and 15% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = snotel_data.drop('snowdensity', axis=1).values\n",
    "targets = snotel_data['snowdensity'].values\n",
    "\n",
    "# Split the dataset into training and temp sets (85% train, 15% temp)\n",
    "features_train, features_temp, targets_train, targets_temp = train_test_split(\n",
    "    features, targets, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Further split the temp set into validation and test sets (15% each)\n",
    "features_val, features_test, targets_val, targets_test = train_test_split(\n",
    "    features_temp, targets_temp, test_size=0.5, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess Data\n",
    "\n",
    "Now that we've split the data, we can apply scaling. The scaler should be fit on the training data and then used to transform the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(features_train)\n",
    "\n",
    "# Transform the training, validation, and test sets\n",
    "features_train = scaler.transform(features_train)\n",
    "features_val = scaler.transform(features_val)\n",
    "features_test = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating Custom Datasets\n",
    "\n",
    "Next, we define custom `Dataset` classes for each of the three sets: training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNOTELDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "\n",
    "# Create instances of the custom datasets for training, validation, and testing sets\n",
    "train_dataset = SNOTELDataset(data=features_train, targets=targets_train)\n",
    "val_dataset = SNOTELDataset(data=features_val, targets=targets_val)\n",
    "test_dataset = SNOTELDataset(data=features_test, targets=targets_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Using DataLoader\n",
    "\n",
    "Now, we use `DataLoader` to manage our data in mini-batches during training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, validation, and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "We define a simple feedforward neural network using `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNOTELNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNOTELNN, self).__init__() # super class to inherit from nn.Module\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer 1\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Fully connected layer 2\n",
    "    \n",
    "    def forward(self, x): # x is the batch of input\n",
    "        # Define the forward pass\n",
    "        out = self.fc1(x)  # Pass input through first layer\n",
    "        out = self.relu(out)  # Apply ReLU activation\n",
    "        out = self.fc2(out)  # Pass through second layer to get output\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "# Instantiate the model and move it to the device (GPU or CPU)\n",
    "model = SNOTELNN(input_size=features_train.shape[1], hidden_size=128, output_size=1).to(available_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method defines how the input data flows through the network layers. It specifies the sequence of operations that the data undergoes as it moves from the input layer to the output layer. This method is automatically called when you pass data through the model (e.g., `outputs = model(inputs)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Loss Function and Optimizer\n",
    "\n",
    "For this example, we’ll use Mean Squared Error Loss since we’re dealing with a regression problem. We’ll use the Adam optimizer, which is a good default choice due to its adaptive learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "We now write the training loop, which includes zeroing the gradients, performing the forward pass, computing the loss, backpropagating, and updating the model parameters. We will also validate the model on the validation set after each epoch.\n",
    "\n",
    "```{note}\n",
    "An **Epoch** refers to one complete pass through the entire training dataset. During each epoch, the model sees every example in the dataset once.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0  # Initialize cumulative training loss\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to the appropriate device\n",
    "        inputs, labels = inputs.to(available_device), labels.to(available_device)\n",
    "        \n",
    "        # Zero the gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Perform backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Average training loss\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)  # Store the training loss for this epoch\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(available_device), labels.to(available_device)  # Move to device\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Average validation loss\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)  # Store the validation loss for this epoch\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set and collect predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0  # Initialize cumulative test loss\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move data to the appropriate device\n",
    "        inputs, labels = inputs.to(available_device), labels.to(available_device)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Accumulate test loss\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Store the predictions and the corresponding labels\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate the average test loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Convert lists to numpy arrays for plotting\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Plot observed vs predicted\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(all_labels, all_preds, alpha=0.7)\n",
    "plt.xlabel('Observed (Actual) Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Observed vs. Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Saving your trained model is an essential part of any machine learning project. It allows you to reuse the model for predictions, further training, or sharing with others without having to retrain it from scratch. In PyTorch, saving and loading models is straightforward and can be done using the `torch.save` and `torch.load` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'snotel_nn_model.pth')\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = SNOTELNN(input_size=features_train.shape[1], hidden_size=128, output_size=1)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(torch.load('snotel_nn_model.pth', weights_only=True))\n",
    "\n",
    "# Set the model to evaluation mode before inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is a critical step in building machine learning models. Unlike model parameters (like weights and biases), which are learned from the data during training, hyperparameters are the settings you choose before the training process begins. These include:\n",
    "\n",
    "- **Learning Rate**: Controls how much to adjust the model’s weights with respect to the loss gradient.\n",
    "- **Batch Size**: Determines the number of training examples utilized in one iteration.\n",
    "- **Number of Hidden Layers and Neurons**: Specifies the architecture of the neural network.\n",
    "- **Optimizer**: The algorithm used to update model weights based on the computed gradients (e.g., Adam, SGD).\n",
    "\n",
    "\n",
    "Tuning these hyperparameters can significantly affect the performance of your model. However, finding the optimal set of hyperparameters can be a challenging and time-consuming process, often requiring experimentation.\n",
    "\n",
    "### Manual vs. Automated Tuning\n",
    "\n",
    "- **Manual Tuning**: Involves adjusting hyperparameters based on intuition, experience, or trial and error. While straightforward, this approach can be inefficient and might not always yield the best results.\n",
    "- **Automated Tuning**: Tools like **Optuna** can help automate the search for the best hyperparameters. These tools explore the hyperparameter space more systematically and can save a lot of time compared to manual tuning. Sample PyTorch hyperparameter tuning for Optuna can be found [here](https://github.com/optuna/optuna-examples/tree/main/pytorch).\n",
    "\n",
    "### Further Reading and Tools\n",
    "\n",
    "Since hyperparameter tuning is a vast topic and we have limited time, I recommend exploring the following resources and tools for a deeper dive\n",
    "\n",
    "* Optuna: [documentation](https://optuna.org/)\n",
    "* Ray Tune:  A scalable hyperparameter tuning library, particularly useful if you need to distribute tuning across multiple machines. See [documentation](https://docs.ray.io/en/latest/tune/index.html) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* Many thanks to HP Marshall (my advisor) for his mentorship and support. \n",
    "* Many thanks to e-Science institute and all organizing members for allowing me deploy/present this tutorial. A huge thanks to eveyone for listening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n",
    "2. [Machine Learning Bookcamp](https://www.manning.com/books/machine-learning-bookcamp?query=machine)\n",
    "3. [An Introduction to Statistical Learning with Applications in R](https://link.springer.com/book/10.1007%2F978-1-4614-7138-7) (available online for free)\n",
    "4. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "[Ensemble Methods for Machine Learning](https://www.manning.com/books/ensemble-methods-for-machine-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
